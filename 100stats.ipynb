{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import xlsxwriter\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropTopLeftRight(spec):\n",
    "\n",
    "    # finding elements from varsNamesSet\n",
    "    start_norm = time.time()\n",
    "    spec = spec.replace('\\s+', ' ', regex=True).astype(str).apply(lambda x: x.str.lower())\n",
    "    spec = spec.replace('nan', np.nan)\n",
    "    end_norm = time.time()\n",
    "#     print('Таблица нормализована за: {}'.format(end_norm-start_norm))\n",
    "    normTime = (end_norm-start_norm)\n",
    "\n",
    "    start_norm = time.time()\n",
    "    specTrueFalseMap = spec.isin(varsNamesSet).replace(False, np.nan)\n",
    "    if specTrueFalseMap.count(axis='columns').sum() == 0:\n",
    "#         print('Не удалось обработать файл')\n",
    "        return  pd.DataFrame()\n",
    "\n",
    "\n",
    "    # находим строку\n",
    "    heading_row = specTrueFalseMap.count(axis='columns').idxmax()+2\n",
    "    # находим левую границу\n",
    "    a = spec.iloc[specTrueFalseMap.count(axis='columns').idxmax()].isnull().replace(True,1)\n",
    "    if 'Unnamed: ' in a.idxmin():\n",
    "        heading_left_end = int(a.idxmin().split('Unnamed: ')[1])+1\n",
    "    else:\n",
    "        heading_left_end = a.index.get_loc(a.idxmin())+1\n",
    "    # находим правую границу\n",
    "    min_a = min(list(a))\n",
    "    len_a = len(list(a))\n",
    "    if [i for i, j in enumerate(list(a)) if j == min_a][-1] < len_a-1:\n",
    "        heading_right_end = [i for i, j in enumerate(list(a)) if j == min_a][-1]+1\n",
    "    else:\n",
    "        heading_right_end = len_a\n",
    "\n",
    "    RC_title = ((heading_row,heading_left_end),(heading_row,heading_right_end))\n",
    "\n",
    "    # находим строку с номерами столбцов\n",
    "    # в doesContainNumbersRow тупл с bool и координатами(тоже тупл)\n",
    "    doesContainNumbersRow = (False,())\n",
    "\n",
    "    try:\n",
    "        if int(spec.iloc[heading_row-1][1])+1 == int(spec.iloc[heading_row-1][2]):\n",
    "            doesContainNumbersRow = (True,((heading_row+1,heading_left_end),(heading_row+1,heading_right_end)))\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if not doesContainNumbersRow[0]:\n",
    "        try:\n",
    "            if int(spec.iloc[heading_row][1])+1 == int(spec.iloc[heading_row][2]):\n",
    "                doesContainNumbersRow = (True,((heading_row+2,heading_left_end),(heading_row+2,heading_right_end)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # находим количество заголовков\n",
    "    numberOfTitleNames = (~spec.iloc[heading_row-2].isnull()).replace(True,1).sum()\n",
    "\n",
    "    # находим повторяющиеся заголовки и их количество\n",
    "    titelNames = Counter(spec.iloc[heading_row-2])\n",
    "    repeatedTitleNames = Counter(el for el in titelNames.elements() if titelNames[el] >= 2)\n",
    "#     print(repeatedTitleNames)\n",
    "    amountOFRepeatedTitleNames = sum(repeatedTitleNames.values())\n",
    "\n",
    "\n",
    "    # выясняем двойной ли заголовок\n",
    "    titleIsDoubled = False\n",
    "\n",
    "    if doesContainNumbersRow[0]:\n",
    "        if heading_row == doesContainNumbersRow[1][0][0]-2:\n",
    "            titleIsDoubled = True\n",
    "    else:\n",
    "        smallHead = list(spec.iloc[heading_row-1:].head(3).count(axis=1))\n",
    "        if smallHead[0] < sum(smallHead)/len(smallHead):\n",
    "            titleIsDoubled = True\n",
    "\n",
    "    # находим вторую (английскую) строку заголовков\n",
    "    secondTitle = (False,())\n",
    "\n",
    "    if titleIsDoubled:\n",
    "        try:\n",
    "            if (~spec.iloc[heading_row-4].isnull()).replace(True,1).sum() > 3:\n",
    "                secondTitle = (True, ((heading_row-2,heading_left_end),(heading_row-2,heading_right_end)))\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        try:\n",
    "            if (~spec.iloc[heading_row-3].isnull()).replace(True,1).sum() > 3:\n",
    "                secondTitle = (True, ((heading_row-1,heading_left_end),(heading_row-1,heading_right_end)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # # дропаем строку с номерами\n",
    "    # if doesContainNumbersRow[0]:\n",
    "    #     if doesContainNumbersRow[1][0][0] - heading_row == 1:\n",
    "    #         spec.drop(heading_row+1, inplace = True)\n",
    "    #     elif doesContainNumbersRow[1][0][0] - heading_row == 1:\n",
    "    #         spec.drop(heading_row, inplace = True)\n",
    "\n",
    "    spec = spec.iloc[specTrueFalseMap.count(axis='columns').idxmax():].dropna(how='all')\n",
    "    # print(spec)\n",
    "    listOfEmptyTitleFields = [i for i, j in spec.iloc[0].isnull().replace(False,np.nan).to_dict().items() if j == 1]\n",
    "    spec = spec.drop(columns=listOfEmptyTitleFields)\n",
    "    spec.columns = spec.iloc[0]\n",
    "    spec = spec.iloc[1:]\n",
    "    spec.reset_index(inplace = True,drop=True)\n",
    "    spec.columns.name = ''\n",
    "\n",
    "    end_norm = time.time()\n",
    "#     print('Чтение диапазона заголовков выполнено за: {}'.format(end_norm-start_norm))\n",
    "    titleReadingTime = end_norm-start_norm\n",
    "    return spec, RC_title, numberOfTitleNames, repeatedTitleNames, amountOFRepeatedTitleNames, titleIsDoubled, secondTitle,doesContainNumbersRow, titleReadingTime, normTime\n",
    "\n",
    "def renameColumnsNames(spec):\n",
    "    start_norm = time.time()\n",
    "    changedColumnsNames = []\n",
    "    changedColumnsNamesDict = {}\n",
    "\n",
    "    for i,col in enumerate(spec):\n",
    "        if col in varsNamesSet:\n",
    "            for j in varsDF:\n",
    "                if col in list(varsDF[j]):\n",
    "                    spec.rename(columns={col: j}, inplace=True)\n",
    "                    changedColumnsNames.append(j)\n",
    "                    try:\n",
    "                        if col in list(copy_columns[j]):\n",
    "                            changedColumnsNamesDict[col] = ((j,'copy_columns'))\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        if col in list(text_columns[j]):\n",
    "                            changedColumnsNamesDict[col] = ((j,'text_columns'))\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        if col in list(eng_vars_columns[j]):\n",
    "                            changedColumnsNamesDict[col] = ((j,'eng_vars_columns'))\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    break\n",
    "        else:\n",
    "            changedColumnsNamesDict[col] = (('не обработан',''))\n",
    "\n",
    "\n",
    "\n",
    "    # numberOfColumnsAppearences = Counter(changedColumnsNames)\n",
    "    changedColumnsNames = list(set(changedColumnsNames))\n",
    "    end_norm = time.time()\n",
    "#     print('Обработка диапазона заголовков рабочей таблицы выполнена за: {}'.format(end_norm-start_norm))\n",
    "    return spec, changedColumnsNames, changedColumnsNamesDict\n",
    "\n",
    "def dropSpecsBottom(spec,titleIsDoubled,doesContainNumbersRow):\n",
    "    apearencesOfNanInRows = spec.isnull().sum(axis=1)\n",
    "\n",
    "    # h = Counter(apearencesOfNanInRows.head(int(len(apearencesOfNanInRows)/2))).most_common()\n",
    "\n",
    "    # listOfRowIndicesToDelete = list(apearencesOfNanInRows.loc[apearencesOfNanInRows > 1 + h[0][0]].index)\n",
    "\n",
    "\n",
    "    if doesContainNumbersRow[0] &  titleIsDoubled:\n",
    "        apearenceOfNanInRows = spec.iloc[2].isnull().sum()\n",
    "        # spec.drop([0,1], inplace=True)\n",
    "    elif doesContainNumbersRow[0] | titleIsDoubled:\n",
    "        apearenceOfNanInRows = spec.iloc[1].isnull().sum()\n",
    "        # spec.drop(0, inplace=True)\n",
    "    else:\n",
    "        apearenceOfNanInRows = spec.iloc[0].isnull().sum()\n",
    "\n",
    "    # apearenceOfNanInRows = spec.iloc[0].isnull().sum()\n",
    "    listOfRowIndicesToDelete = list(apearencesOfNanInRows.loc[apearencesOfNanInRows > 1 + apearenceOfNanInRows].index)\n",
    "\n",
    "    spec.drop(listOfRowIndicesToDelete,inplace=True)\n",
    "\n",
    "    if doesContainNumbersRow[0] &  titleIsDoubled:\n",
    "        # apearenceOfNanInRows = spec.iloc[2].isnull().sum()\n",
    "        try:\n",
    "            spec.drop([0,1], inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "    elif doesContainNumbersRow[0] | titleIsDoubled:\n",
    "        # apearenceOfNanInRows = spec.iloc[1].isnull().sum()\n",
    "        try:\n",
    "            spec.drop(0, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return spec\n",
    "\n",
    "def findSameColumnNames(spec):\n",
    "    # если есть одноименные столбцы мы их мержим. в astype можно указать другой тип данных\n",
    "    # к сожалению этот код меняет порядок в столбцов\n",
    "    def sjoin(x): return ';'.join(x[x.notnull()].astype(str))\n",
    "    spec = spec.groupby(level=0, axis=1).apply(lambda x: x.apply(sjoin, axis=1))\n",
    "    return spec\n",
    "\n",
    "def changingColumnsValues(spec):\n",
    "    columnsData = {}\n",
    "\n",
    "    for (columnName, columnData) in spec.iteritems():\n",
    "        columnsData[columnName] = {}\n",
    "        columnsData[columnName]['amountOfEmptyFields'] = spec[columnName].isnull().sum()\n",
    "        columnsData[columnName]['amountOfChangedFields'] = 0\n",
    "        columnsData[columnName]['amountOfUnchangedFields'] = len(columnData)\n",
    "        columnsData[columnName]['unchangedFields'] = set()\n",
    "\n",
    "        if columnName in list(copy_columns.columns):\n",
    "            columnsData[columnName]['amountOfChangedFields'] = columnData.astype(str).str.contains(',', regex=False).replace(np.nan, False).sum()\n",
    "            columnsData[columnName]['amountOfUnchangedFields'] = len(columnData) - columnsData[columnName]['amountOfChangedFields']\n",
    "            # print(spec[columnName].loc[~(spec[columnName].astype(str).str.contains(',', regex=False).replace(np.nan, True))])\n",
    "            # print(set(spec[columnName].loc[~(spec[columnName].astype(str).str.contains(',', regex=False).replace(np.nan, True))]))\n",
    "            columnsData[columnName]['unchangedFields'] = set(spec[columnName].loc[~(spec[columnName].astype(str).str.contains(',', regex=False).replace(np.nan, True))])\n",
    "            spec[columnName] = spec[columnName].astype(str).str.replace(',','.')\n",
    "\n",
    "        elif columnName in list(eng_vars_columns.columns):\n",
    "    #         for val in columnData:\n",
    "            for k in eng_vars_Connection_columns.keys():\n",
    "                columnsData[columnName]['unchangedFields'] = columnsData[columnName]['unchangedFields'].union(set(spec[columnName].loc[~spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)]))\n",
    "                columnsData[columnName]['amountOfChangedFields'] += len(spec[columnName].loc[spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)])\n",
    "                columnsData[columnName]['amountOfUnchangedFields'] -= len(spec[columnName].loc[spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)])\n",
    "                spec[columnName] = spec[columnName].astype(str).str.replace(k,str(eng_vars_Connection_columns[k][0]),regex=False)\n",
    "            for k in eng_vars_Material_columns.keys():\n",
    "                columnsData[columnName]['unchangedFields'] = columnsData[columnName]['unchangedFields'].union(set(spec[columnName].loc[~spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)]))\n",
    "                columnsData[columnName]['amountOfChangedFields'] += len(spec[columnName].loc[spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)])\n",
    "                columnsData[columnName]['amountOfUnchangedFields'] -= len(spec[columnName].loc[spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)])\n",
    "                spec[columnName] = spec[columnName].astype(str).str.replace(k,str(eng_vars_Material_columns[k][0]),regex=False)\n",
    "            for k in eng_vars_SeismoCat_columns.keys():\n",
    "                columnsData[columnName]['unchangedFields'] = columnsData[columnName]['unchangedFields'].union(set(spec[columnName].loc[~spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)]))\n",
    "                columnsData[columnName]['amountOfChangedFields'] += len(spec[columnName].loc[spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)])\n",
    "                columnsData[columnName]['amountOfUnchangedFields'] -= len(spec[columnName].loc[spec[columnName].astype(str).str.contains(k, regex=False).replace(np.nan, True)])\n",
    "                spec[columnName] = spec[columnName].astype(str).str.replace(k,str(eng_vars_SeismoCat_columns[k][0]),regex=False)\n",
    "\n",
    "            columnsData[columnName]['unchangedFields'] = list(set(columnsData[columnName]['unchangedFields']))\n",
    "        # print('{}: {} : {}'.format(columnName, columnsData[columnName]['unchangedFields'], len(columnsData[columnName]['unchangedFields']) ))\n",
    "    return spec, columnsData\n",
    "\n",
    "def findingTable(spec):\n",
    "    stats = {}\n",
    "    # droppig unwanted columns and rows from top and left\n",
    "    spec, stats['RC_title'], stats['numberOfTitleNames'], stats['repeatedTitleNames'], stats['amountOFRepeatedTitleNames'], stats['titleIsDoubled'], stats['secondTitle'],stats['doesContainNumbersRow'],stats['titleReadingTime'], stats['normTime'] = dropTopLeftRight(spec)\n",
    "\n",
    "    # changing columns names\n",
    "    spec, changedColumnsNames, stats['changedColumnsNamesDict'] = renameColumnsNames(spec)\n",
    "    stats['changedColumnsNamesAmount'] = len(changedColumnsNames)\n",
    "\n",
    "\n",
    "    # finding the bottom of spec\n",
    "    start_read = time.time()\n",
    "    spec = dropSpecsBottom(spec,stats['titleIsDoubled'],stats['doesContainNumbersRow'])\n",
    "\n",
    "    # находим диапазон значений\n",
    "    stats['RC_rows'] = ( ( stats['RC_title'][0][0] + 1 , stats['RC_title'][0][1] ) , ( stats['RC_title'][0][0] + 1 , stats['RC_title'][1][1] ) )\n",
    "\n",
    "    if stats['doesContainNumbersRow'][0] &  stats['titleIsDoubled']:\n",
    "        stats['RC_rows'] = ((stats['RC_title'][0][0]+3,stats['RC_title'][0][1]),(spec.tail(1).index[0] + stats['RC_title'][0][0] + 1,stats['RC_title'][1][1]))\n",
    "    elif stats['doesContainNumbersRow'][0] | stats['titleIsDoubled']:\n",
    "        if stats['doesContainNumbersRow'][0]:\n",
    "            stats['RC_rows'] = ((stats['RC_title'][0][0]+2,stats['RC_title'][0][1]),(spec.tail(1).index[0] + stats['RC_title'][0][0] + 1,stats['RC_title'][1][1]))\n",
    "        else:\n",
    "            stats['RC_rows'] = ((stats['RC_title'][0][0]+2,stats['RC_title'][0][1]),(spec.tail(1).index[0] + stats['RC_title'][0][0] + 1,stats['RC_title'][1][1]))\n",
    "\n",
    "\n",
    "    stats['amountOfRows'] = stats['RC_rows'][1][0] - stats['RC_rows'][0][0] + 1\n",
    "\n",
    "    end_read = time.time()\n",
    "#     print('Чтение диапазона характеристик выполнено за: {}'.format(end_read-start_read))\n",
    "    stats['chReadingTime'] = end_read-start_read\n",
    "\n",
    "    spec = spec.fillna('REPLACEMENT')\n",
    "\n",
    "    # find and merge columns with same name\n",
    "    spec = findSameColumnNames(spec)\n",
    "    copiesFreqDF = pd.DataFrame(spec.value_counts(subset=changedColumnsNames))\n",
    "\n",
    "    spec['countClones'] = 0\n",
    "    for i,r in enumerate(spec.iterrows()):\n",
    "        spec['countClones'].iloc[i] = copiesFreqDF.loc[tuple(r[1].loc[changedColumnsNames])][0]\n",
    "        copiesFreqDF.loc[tuple(r[1].loc[changedColumnsNames])][0] = 0\n",
    "\n",
    "    spec = spec.loc[spec['countClones']!=0]\n",
    "\n",
    "    if 'DN' in spec.columns:\n",
    "        spec = spec.sort_values(by=['DN'])\n",
    "\n",
    "    spec = spec.replace('REPLACEMENT', np.nan)\n",
    "\n",
    "    spec , stats['columnsStats']= changingColumnsValues(spec)\n",
    "    return spec, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-fe15245abe2a>:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  eng_vars_Connection_columns = eng_vars_Connection_columns.set_index('connection_ru').T.to_dict('list')\n",
      "<ipython-input-8-fe15245abe2a>:15: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  eng_vars_Material_columns = eng_vars_Material_columns.set_index('material_ru').T.to_dict('list')\n",
      "<ipython-input-8-fe15245abe2a>:19: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  eng_vars_SeismoCat_columns = eng_vars_SeismoCat_columns.set_index('SeismoCat_ru').T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "xls = pd.ExcelFile('vars_for_columns.xlsx')\n",
    "\n",
    "copy_columns = pd.read_excel(xls, 'copy_columns').replace('\\s+', ' ', regex=True).astype(str).apply(lambda x: x.str.lower())\n",
    "text_columns = pd.read_excel(xls, 'text_columns').replace('\\s+', ' ', regex=True).astype(str).apply(lambda x: x.str.lower())\n",
    "eng_vars_columns = pd.read_excel(xls, 'eng_vars_columns').replace('\\s+', ' ', regex=True).astype(str).apply(lambda x: x.str.lower())\n",
    "\n",
    "eng_vars_Connection_columns = pd.read_excel('english_vars_Connection.xlsx').replace('\\s+', ' ', regex=True)\n",
    "eng_vars_Connection_columns.connection_ru = eng_vars_Connection_columns.connection_ru.str.lower()\n",
    "eng_vars_Connection_columns = eng_vars_Connection_columns[['connection_ru','connection']]\n",
    "eng_vars_Connection_columns = eng_vars_Connection_columns.set_index('connection_ru').T.to_dict('list')\n",
    "\n",
    "eng_vars_Material_columns = pd.read_excel('english_vars_Material.xlsx').replace('\\s+', ' ', regex=True)\n",
    "eng_vars_Material_columns.material_ru = eng_vars_Material_columns.material_ru.str.lower()\n",
    "eng_vars_Material_columns = eng_vars_Material_columns[['material_ru','material']]\n",
    "eng_vars_Material_columns = eng_vars_Material_columns.set_index('material_ru').T.to_dict('list')\n",
    "\n",
    "eng_vars_SeismoCat_columns = pd.read_excel('english_vars_SeismoCat.xlsx').replace('\\s+', ' ', regex=True).astype(str).apply(lambda x: x.str.lower())\n",
    "eng_vars_SeismoCat_columns = eng_vars_SeismoCat_columns[['SeismoCat_ru','SeismoCat']]\n",
    "eng_vars_SeismoCat_columns = eng_vars_SeismoCat_columns.set_index('SeismoCat_ru').T.to_dict('list')\n",
    "\n",
    "varsDF = pd.concat([copy_columns,text_columns,eng_vars_columns],axis=1)\n",
    "\n",
    "varsNamesSet = set(copy_columns.stack().tolist())|set(text_columns.stack().tolist())|set(eng_vars_columns.stack().tolist())\n",
    "varsNamesSet.discard('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unchangedFields = {}\n",
    "titleNames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '100specs'\n",
    "\n",
    "for entry in os.listdir(PATH):\n",
    "    if os.path.isfile(os.path.join(PATH, entry)):\n",
    "        if (entry.split('.')[-1] == 'xls' )|(entry.split('.')[-1] == 'xlsx' ):\n",
    "            spec = pd.read_excel(os.path.join(PATH, entry))\n",
    "            spec, stats = findingTable(spec)\n",
    "            for stat in stats['columnsStats']:\n",
    "                if stat in unchangedFields.keys():\n",
    "                    unchangedFields[stat] = unchangedFields[stat].union(stats['columnsStats'][stat]['unchangedFields'])\n",
    "                else:\n",
    "                    unchangedFields[stat] = set(stats['columnsStats'][stat]['unchangedFields'])\n",
    "                \n",
    "            for t in stats['changedColumnsNamesDict']:\n",
    "                if t not in titleNames.keys():\n",
    "                    titleNames[t] = stats['changedColumnsNamesDict'][t]\n",
    "                        \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('table_'+PATH.split('/')[-1]+'.xlsx', engine='xlsxwriter')\n",
    "\n",
    "\n",
    "# Get the xlsxwriter workbook and worksheet objects.\n",
    "workbook  = writer.book\n",
    "worksheet2 = workbook.add_worksheet()\n",
    "worksheet3 = workbook.add_worksheet()\n",
    "\n",
    "\n",
    "row = 2\n",
    "for key in titleNames.keys():\n",
    "\n",
    "    worksheet2.write('A' + str(row), key)\n",
    "    worksheet2.write('B' + str(row), str(titleNames[key]))\n",
    "    row = row + 1\n",
    "\n",
    "\n",
    "col = 2\n",
    "\n",
    "listOfUnchangedValues = set()\n",
    "\n",
    "def isNaN(num):\n",
    "    return num != num\n",
    "\n",
    "for key in unchangedFields.keys():\n",
    "    worksheet3.write(2, col, key)\n",
    "    worksheet3.write(3, col, 'unchangedFields')\n",
    "    k = 0\n",
    "    for el in unchangedFields[key]:\n",
    "        if not isNaN(el):\n",
    "            worksheet3.write(3+k, col+1,el)\n",
    "            k += 1\n",
    "\n",
    "    col += 2\n",
    "\n",
    "\n",
    "# print(listOfUnchangedValues)\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sls_env",
   "language": "python",
   "name": "sls_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
